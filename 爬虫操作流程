基本操作流程:
    # Terminal 下输入
    1.创建项目  scrapy startproject 项目名称
    2.进入项目目录  cd 项目名称
    3.创建爬虫: 方法一  scrapy genspider -t crawl 爬虫名  爬取的网站(不要加http)
               方法二  scrapy genspider 爬虫名 爬取网址
    4. 创建启动爬虫文件 main.py  每次需在此文件下运行
    5. 配置settings.py文件:
        1.ROBOTSTXT_OBEY = True  ————— 是否遵守robots.txt规则  设置为 Fales
        2.CONCURRENT_REQUESTS = 16 --- 开启线程数量，默认16，可以自行设置
                                这个参数涉及到scrapy爬取的并发量，items的处理速度
        3.DOWNLOAD_DELAY = 3     ———  下载延迟时间。
        4.CONCURRENT_REQUESTS_PER_DOMAIN = 16   将对任何单个域执行的并发（即同时）请求的最大数量。
        5.COOKIES_ENABLED = False
            是否启用cookie。是否启用cookies middleware。如果关闭，cookies将不会发送给web server。
        6.COOKIES_DEBUG = False
            自行添加
            如果启用，Scrapy将记录所有在request(cookie 请求头)发送的cookies及response接收到的cookies（set-cookie接收头）
        7.AUTOTHROTTLE_START_DELAY = 5  初始下载延迟时间(单位：秒)
        8.AUTOTHROTTLE_MAX_DELAY = 60   高并发请求时最大延迟时间(单位：秒)
  ***** 9.USER_AGENT  用户代理   需在settings文件中先配置代理池,再去middlewares里编写调用类方法
  ***** 10.DOWNLOADER_MIDDLEWARES
             要激活下载器中间件组件，将其加入到 DOWNLOADER_MIDDLEWARES 设置中。
             该设置是一个字典(dict)，键为中间件类的路径，值为其中间件的顺序(order)。
  ***** 11. ITEM_PIPELINES  下载图片,同步数据库时需要到此处注册中间件
            IMAGES_STORE = '图片保存的绝对路径'
            #  设置保存图片的大小
            IMAGES_THUMBS={
                #"small":(50, 20),
                #"big":(270, 270),
                "big":(500, 500),
            }